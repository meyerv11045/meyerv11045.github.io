<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.21.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Logistic Regression - The Learning Journey</title>
<meta name="description" content="While linear regression makes continuous value predictions (e.g. any number between 0 and 100), logistic regression makes discrete predictions (e.g. $y$ is either 1 or 0). These discrete predictions can be used to classify inputs such as whether an email is spam or not spam or whether a tumor is malign or benign. In order to achieve this binary output, the hypothesis for linear regression must be slightly modified so that it maps all values of $\theta^TX$ to between 0 and 1 so that they can be treated as probabilities. This mapping is achieved using the sigmoid or logistic function ($\sigma(z)$) whose equation and graph is shown below.">


  <meta name="author" content="Vikram Meyer">
  
  <meta property="article:author" content="Vikram Meyer">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="The Learning Journey">
<meta property="og:title" content="Logistic Regression">
<meta property="og:url" content="http://localhost:4000/machinelearning/lesson11/">


  <meta property="og:description" content="While linear regression makes continuous value predictions (e.g. any number between 0 and 100), logistic regression makes discrete predictions (e.g. $y$ is either 1 or 0). These discrete predictions can be used to classify inputs such as whether an email is spam or not spam or whether a tumor is malign or benign. In order to achieve this binary output, the hypothesis for linear regression must be slightly modified so that it maps all values of $\theta^TX$ to between 0 and 1 so that they can be treated as probabilities. This mapping is achieved using the sigmoid or logistic function ($\sigma(z)$) whose equation and graph is shown below.">







  <meta property="article:published_time" content="2020-12-16T22:56:36-05:00">





  

  


<link rel="canonical" href="http://localhost:4000/machinelearning/lesson11/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Vikram Meyer",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="The Learning Journey Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/profile.png" alt=""></a>
        
        <a class="site-title" href="/">
          The Learning Journey
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About Me</a>
            </li><li class="masthead__menu-item">
              <a href="/machinelearning/">Machine Learning</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">Artificial Intelligence</span>
        

        
        <ul>
          
            <li><a href="/machinelearning/lesson1/">What is Artificial Intelligence?</a></li>
          
            <li><a href="/machinelearning/lesson2/">Classifications of AI</a></li>
          
            <li><a href="/machinelearning/lesson3/">AI vs. ML vs. DL</a></li>
          
            <li><a href="/machinelearning/lesson4/">What is Machine Learning?</a></li>
          
            <li><a href="/machinelearning/lesson5/">3 Types of Machine Learning</a></li>
          
            <li><a href="/machinelearning/lesson6/">What is Deep Learning?</a></li>
          
            <li><a href="/machinelearning/lesson7/">ML vs. DL- Defining Features</a></li>
          
            <li><a href="/machinelearning/lesson8/">End Note</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Supervised Learning Algorithms</span>
        

        
        <ul>
          
            <li><a href="/machinelearning/lesson9/">Overview</a></li>
          
            <li><a href="/machinelearning/lesson10/">Linear Regression</a></li>
          
            <li><a href="/machinelearning/lesson11/" class="active">Logistic Regression</a></li>
          
            <li><a href="/machinelearning/lesson12/">Gradient Descent</a></li>
          
            <li><a href="/machinelearning/lesson13/">Gradient Descent Optimization Algorithms</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Neural Networks</span>
        

        
        <ul>
          
            <li><a href="/machinelearning/lesson14/">Overview</a></li>
          
            <li><a href="/machinelearning/lesson15/">Activation Functions</a></li>
          
            <li><a href="/machinelearning/lesson16/">Backpropagation</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Error Analysis</span>
        

        
        <ul>
          
            <li><a href="/machinelearning/lesson17/">Overview</a></li>
          
            <li><a href="/machinelearning/lesson18/">Overfitting vs. Underfitting Analogy</a></li>
          
            <li><a href="/machinelearning/lesson19/">Overfitting vs. Underfitting</a></li>
          
            <li><a href="/machinelearning/lesson20/">Solutions</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Logistic Regression">
    <meta itemprop="description" content="While linear regression makes continuous value predictions (e.g. any number between 0 and 100), logistic regression makes discrete predictions (e.g. $y$ is either 1 or 0). These discrete predictions can be used to classify inputs such as whether an email is spam or not spam or whether a tumor is malign or benign. In order to achieve this binary output, the hypothesis for linear regression must be slightly modified so that it maps all values of $\theta^TX$ to between 0 and 1 so that they can be treated as probabilities. This mapping is achieved using the sigmoid or logistic function ($\sigma(z)$) whose equation and graph is shown below.">
    <meta itemprop="datePublished" content="2020-12-16T22:56:36-05:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Logistic Regression
</h1>
          


        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>While linear regression makes continuous value predictions (e.g. any number between 0 and 100), logistic regression makes discrete predictions (e.g. $y$ is either 1 or 0). These discrete predictions can be used to classify inputs such as whether an email is spam or not spam or whether a tumor is malign or benign. In order to achieve this binary output, the hypothesis for linear regression must be slightly modified so that it maps all values of $\theta^TX$ to between 0 and 1 so that they can be treated as probabilities. This mapping is achieved using the sigmoid or logistic function ($\sigma(z)$) whose equation and graph is shown below.</p>

\[\begin{equation}
    \sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation}\]

<p><img src="/assets/images/ML_images/SupervisedLearning/sigmoidFn.png" alt="Sigmoid Function" /></p>

<p>The sigmoid function is incorporated into the hypothesis like so:
\(\begin{equation}
    h_\theta(X) = \sigma(\theta^TX) = \frac{1}{1 + e^{-\theta^TX}}
\end{equation}\)</p>

<p>Now that the output of the hypothesis is a probability between 0 and 1, a decision boundary to determine the class of input data:
    if $h_\theta(x) &gt;= 0.5$ then predict $y=1$
    if $h_\theta(x) &lt; 0.5$ then predict $y=0$.</p>

<p>What is happening with this decision boundary is that the hypothesis predicts an output the same way as linear regression and then maps this output to a probability between 0 and 1 unlike linear regression. Then a threshold, 0.5 in most cases, is applied to determine the class (1 or 0) for the input data (classification).</p>

<p>Now that we understand the hypothesis used in logistic regression and how the hypothesis is interpreted to create a decision boundary, we can begin to look at the cost function that will be used by the gradient descent algorithm to find the best parameters in order to minimize the cost function. Logistic regression cannot use the same cost function as linear regression with this binary classification because it would produce a non-convex cost surface with many local minimum, making it much harder to find the global minimum compared to a convex surface \cite{LogisticRegression}. To create a cost function that will produce a convex surface for this binary classification problem, we use two separate cost functions for the two classes ($y=1$ and $y=0$) as seen in equation \ref{piecewiseCost}. Using the different cost functions for the different classes allows incorrect classifications to be highly penalized since the cost function approaches infinity the farther the prediction is from the actual class as seen in the graphs in figure \ref{fig:LogRegCost}. This cost function is referred to as the cross-entropy loss or log loss function.</p>

\[\begin{equation}
\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{array}{ll}
-\log \left(h_{\theta}(x)\right) &amp; \text { if\ } y=1 \\
-\log \left(1-h_{\theta}(x)\right) &amp; \text { if\ } y=0
\end{array}\right.
\end{equation}\]

<p><img src="/assets/images/ML_images/SupervisedLearning/LogisticRegressionCost.png" alt="Logistic Regression Cost" /></p>

<p>In practice, it is much more efficient to remove if-else statements when calculating the cost, so below is the one-line version of the cross-entropy loss function for logistic regression. It utilizes the fact that the label is either 0 or 1 in order to be be equivalent to the cost function expressed below when the values are plugged in. For example, when $y=1$, $J(\theta) = -log(h_\theta(x)$ because the other term goes to 0 when 1 is substituted in. Note that both equation \ref{LogRegCost1} and \ref{LogRegCost2} are equivalent, they just have the negative signs in different places and can be seen both ways in blog posts and documentation.</p>

\[\begin{equation} \label{LogRegCost1}
J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}[y^{(i)} \log (h_{\theta}(x^{(i)}))+(1-y^{(i)}) \log (1-h_{\theta}(x^{(i)}))]
\end{equation}\]

\[\begin{equation} \label{LogRegCost2}
J(\theta)=\frac{1}{m} \sum_{i=1}^{m}[-y^{(i)} \log (h_{\theta}(x^{(i)}))-(1-y^{(i)}) \log (1-h_{\theta}(x^{(i)}))]
\end{equation}\]

<p>With the cost function defined for logistic regression, we can use the gradient descent algorithm discussed in the next section to find the optimal values for the parameter matrix $\theta$ in order to produce the most accurate predictions and the lowest cost.</p>

        
      </section>

      <footer class="page__meta">
        
        


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2020-12-16T22:56:36-05:00">December 16, 2020</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Logistic+Regression%20http%3A%2F%2Flocalhost%3A4000%2Fmachinelearning%2Flesson11%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fmachinelearning%2Flesson11%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fmachinelearning%2Flesson11%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/machinelearning/lesson10/" class="pagination--pager" title="Linear Regression
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://github.com/meyerv11045" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Vikram Meyer. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
