I"¥<p>While linear regression makes continuous value predictions (e.g. any number between 0 and 100), logistic regression makes discrete predictions (e.g. $y$ is either 1 or 0). These discrete predictions can be used to classify inputs such as whether an email is spam or not spam or whether a tumor is malign or benign. In order to achieve this binary output, the hypothesis for linear regression must be slightly modified so that it maps all values of $\theta^TX$ to between 0 and 1 so that they can be treated as probabilities. This mapping is achieved using the sigmoid or logistic function ($\sigma(z)$) whose equation and graph is shown below.</p>

\[\begin{equation}
    \sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation}\]

<p><img src="/assets/images/ML_images/SupervisedLearning/sigmoidFn.png" alt="Sigmoid Function" /></p>

<p>The sigmoid function is incorporated into the hypothesis like so:
\(\begin{equation}
    h_\theta(X) = \sigma(\theta^TX) = \frac{1}{1 + e^{-\theta^TX}}
\end{equation}\)</p>

<p>Now that the output of the hypothesis is a probability between 0 and 1, a decision boundary to determine the class of input data:
    if $h_\theta(x) &gt;= 0.5$ then predict $y=1$
    if $h_\theta(x) &lt; 0.5$ then predict $y=0$.</p>

<p>What is happening with this decision boundary is that the hypothesis predicts an output the same way as linear regression and then maps this output to a probability between 0 and 1 unlike linear regression. Then a threshold, 0.5 in most cases, is applied to determine the class (1 or 0) for the input data (classification).</p>

<p>Now that we understand the hypothesis used in logistic regression and how the hypothesis is interpreted to create a decision boundary, we can begin to look at the cost function that will be used by the gradient descent algorithm to find the best parameters in order to minimize the cost function. Logistic regression cannot use the same cost function as linear regression with this binary classification because it would produce a non-convex cost surface with many local minimum, making it much harder to find the global minimum compared to a convex surface \cite{LogisticRegression}. To create a cost function that will produce a convex surface for this binary classification problem, we use two separate cost functions for the two classes ($y=1$ and $y=0$) as seen in equation \ref{piecewiseCost}. Using the different cost functions for the different classes allows incorrect classifications to be highly penalized since the cost function approaches infinity the farther the prediction is from the actual class as seen in the graphs in figure \ref{fig:LogRegCost}. This cost function is referred to as the cross-entropy loss or log loss function.</p>

\[\begin{equation}
\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{array}{ll}
-\log \left(h_{\theta}(x)\right) &amp; \text { if\ } y=1 \\
-\log \left(1-h_{\theta}(x)\right) &amp; \text { if\ } y=0
\end{array}\right.
\end{equation}\]
:ET