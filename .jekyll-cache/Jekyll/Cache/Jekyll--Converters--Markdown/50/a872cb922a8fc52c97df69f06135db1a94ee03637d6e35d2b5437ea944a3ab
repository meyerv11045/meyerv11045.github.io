I"	<p>As an area of machine learning, supervised learning algorithms are the most basic and will be used to lay the groundwork for machine learning before we dive into the more complex area of reinforcement learning. As mentioned in \ref{SupervisedLearning}, supervised learning involves training a model on labeled data so that it can make accurate predictions on new unlabeled data. The two most basic supervised learning algorithms are linear regression, which is used for regression problems, and logistic regression, which is used for classification problems. Below is a diagram that outlines the basic process for using a supervised learning algorithm to solve a problem. First, a set of labeled data is fed into the machine learning algorithm, which then trains on the data to create a hypothesis $h$ that is able to make accurate predictions $y$ based on an input $x$.</p>

<p><img src="/assets/images/ML_images/SupervisedLearning/ProblemDiagram.png" alt="Problem Diagram" /></p>

<p>Before diving into the specifics of each algorithm, we will define the basic concepts that will be used in both algorithms. The basic groundwork introduced in this section that will be vital to understanding the field of machine learning is the idea of minimizing a cost (a.k.a error or loss) function using an optimization algorithm. In order to learn from labeled data, supervised learning algorithms use a cost function to determine the difference between the predicted output and the actual output for a training example. The algorithm can then use this difference between the prediction and the label to adjust the adjust the parameters so that future predictions are more accurate. Cost functions are denoted as $J(\theta)$ where $\theta$ is a vector of n parameters in the case of linear and logistic regression (e.g. $[\theta_0,\theta_1,\theta_2,â€¦,\theta_n]$).</p>

<p>How does the cost function help the algorithm learn the correct parameters to make accurate predictions? In general, since a less accurate prediction will have a higher cost, the goal is to minimize the cost function $J(\theta)$ by changing the parameters in $\theta$. While there are many methods of minimizing the cost function, the most common in supervised learning is the gradient descent algorithm which will be discussed in detail later on.</p>
:ET